A Mixed-Methods Investigation of Trust Formation in AI-Assisted Decision Making Systems

Principal Investigator: Dr. Sarah Chen
Institution: University of Technology

1. INTRODUCTION

The rapid deployment of artificial intelligence in critical decision-making contexts necessitates 
a deeper understanding of how users develop trust in these systems. This research investigates 
the multifaceted nature of trust formation in AI-assisted decision making.

2. RESEARCH QUESTIONS

RQ1: What individual factors (expertise, prior experience, risk tolerance) influence initial 
trust formation in AI-assisted decision-making systems?

RQ2: How do system characteristics (transparency, accuracy, explainability) moderate the 
relationship between user characteristics and trust development?

RQ3: To what extent does trust in AI systems transfer across different decision-making domains?

RQ4: How does trust evolve over repeated interactions with AI systems, and what factors 
accelerate or inhibit this evolution?

3. THEORETICAL FRAMEWORK

This study draws upon the Technology Acceptance Model (TAM), the Theory of Planned Behavior, 
and emerging frameworks in human-AI interaction to construct a comprehensive model of trust 
formation.

4. HYPOTHESES

H1: Users with higher domain expertise will exhibit lower initial trust in AI recommendations 
than novice users (expertise paradox).

H2: Transparency in AI decision-making processes will have a curvilinear relationship with 
trust, where moderate transparency yields highest trust levels.

H3: Trust built in one AI system will positively transfer to other AI systems within the 
same domain but not across domains.

H4: Users experiencing early success with AI recommendations will show exponential trust 
growth, while early failures will result in persistent trust deficits.

5. METHODOLOGY

Phase 1: Survey Study (n=500)
- Online questionnaire measuring trust propensity, AI experience, and domain expertise
- Validated trust scales adapted for AI context

Phase 2: Experimental Study (n=120)
- 2x3 factorial design: Expertise (High/Low) x Transparency (High/Medium/Low)
- Participants make decisions with AI assistance in financial and medical scenarios
- Trust measured at multiple time points

Phase 3: Longitudinal Field Study (n=50)
- 8-week deployment of AI decision support tool
- Weekly trust assessments and interaction logs
- Exit interviews exploring trust evolution

6. EXPECTED CONTRIBUTIONS

This research will contribute to both theoretical understanding and practical application:
- Refined model of trust formation specific to AI systems
- Design guidelines for trust-calibrated AI interfaces
- Strategies for managing user trust across the AI adoption lifecycle

7. TIMELINE

Months 1-2: IRB approval and survey development
Months 3-4: Survey data collection and analysis
Months 5-7: Experimental study execution
Months 8-10: Longitudinal field study
Months 11-12: Analysis and dissemination